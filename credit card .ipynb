{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc78c4fe",
   "metadata": {},
   "source": [
    "### Importing the denpendencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9d4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1ffd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e29e2",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1e7f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0891d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7   \n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599  \\\n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25   \n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539  \\\n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b6a38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0eb535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f6ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4e7184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3b20b",
   "metadata": {},
   "source": [
    "### Obesrvations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb9f8e",
   "metadata": {},
   "source": [
    "1.We have a dataset of 284807 rows and 31 columns.\n",
    "2.There are no missing values in the data.\n",
    "3.Dataset is highly imbalanced {0:'Normal transaction', 1:'Fradulent transaction'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14cb5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d303612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "credit['Amount'] = rs.fit_transform(credit['Amount'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0856f515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit['Amount'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1b5efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit['Time'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd8b4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "credit['Time'] = mm.fit_transform(credit['Time'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ae7f6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    284807.000000\n",
       "mean          0.548717\n",
       "std           0.274828\n",
       "min           0.000000\n",
       "25%           0.313681\n",
       "50%           0.490138\n",
       "75%           0.806290\n",
       "max           1.000000\n",
       "Name: Time, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit['Time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31d54c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169876</th>\n",
       "      <td>0.693938</td>\n",
       "      <td>-0.611712</td>\n",
       "      <td>-0.769705</td>\n",
       "      <td>-0.149759</td>\n",
       "      <td>-0.224877</td>\n",
       "      <td>2.028577</td>\n",
       "      <td>-2.019887</td>\n",
       "      <td>0.292491</td>\n",
       "      <td>-0.523020</td>\n",
       "      <td>0.358468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075208</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.380739</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>-2.220686</td>\n",
       "      <td>-0.201146</td>\n",
       "      <td>0.066501</td>\n",
       "      <td>0.221180</td>\n",
       "      <td>-0.282401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127467</th>\n",
       "      <td>0.453377</td>\n",
       "      <td>-0.814682</td>\n",
       "      <td>1.319219</td>\n",
       "      <td>1.329415</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>-0.284871</td>\n",
       "      <td>-0.653985</td>\n",
       "      <td>0.321552</td>\n",
       "      <td>0.435975</td>\n",
       "      <td>-0.704298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128619</td>\n",
       "      <td>-0.368565</td>\n",
       "      <td>0.090660</td>\n",
       "      <td>0.401147</td>\n",
       "      <td>-0.261034</td>\n",
       "      <td>0.080621</td>\n",
       "      <td>0.162427</td>\n",
       "      <td>0.059456</td>\n",
       "      <td>-0.279746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137900</th>\n",
       "      <td>0.476770</td>\n",
       "      <td>-0.318193</td>\n",
       "      <td>1.118618</td>\n",
       "      <td>0.969864</td>\n",
       "      <td>-0.127052</td>\n",
       "      <td>0.569563</td>\n",
       "      <td>-0.532484</td>\n",
       "      <td>0.706252</td>\n",
       "      <td>-0.064966</td>\n",
       "      <td>-0.463271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305402</td>\n",
       "      <td>-0.774704</td>\n",
       "      <td>-0.123884</td>\n",
       "      <td>-0.495687</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>0.121679</td>\n",
       "      <td>0.249050</td>\n",
       "      <td>0.092516</td>\n",
       "      <td>-0.294977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21513</th>\n",
       "      <td>0.183556</td>\n",
       "      <td>-1.328271</td>\n",
       "      <td>1.018378</td>\n",
       "      <td>1.775426</td>\n",
       "      <td>-1.574193</td>\n",
       "      <td>-0.117696</td>\n",
       "      <td>-0.457733</td>\n",
       "      <td>0.681867</td>\n",
       "      <td>-0.031641</td>\n",
       "      <td>0.383872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220815</td>\n",
       "      <td>-0.419013</td>\n",
       "      <td>-0.239197</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.232829</td>\n",
       "      <td>0.814177</td>\n",
       "      <td>0.098797</td>\n",
       "      <td>-0.004273</td>\n",
       "      <td>-0.084119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134700</th>\n",
       "      <td>0.468326</td>\n",
       "      <td>1.276712</td>\n",
       "      <td>0.617120</td>\n",
       "      <td>-0.578014</td>\n",
       "      <td>0.879173</td>\n",
       "      <td>0.061706</td>\n",
       "      <td>-1.472002</td>\n",
       "      <td>0.373692</td>\n",
       "      <td>-0.287204</td>\n",
       "      <td>-0.084482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160161</td>\n",
       "      <td>-0.430404</td>\n",
       "      <td>-0.076738</td>\n",
       "      <td>0.258708</td>\n",
       "      <td>0.552170</td>\n",
       "      <td>0.370701</td>\n",
       "      <td>-0.034255</td>\n",
       "      <td>0.041709</td>\n",
       "      <td>-0.296793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21440</th>\n",
       "      <td>0.183261</td>\n",
       "      <td>-2.986845</td>\n",
       "      <td>-8.663978</td>\n",
       "      <td>-1.910863</td>\n",
       "      <td>0.664058</td>\n",
       "      <td>-3.934875</td>\n",
       "      <td>0.861269</td>\n",
       "      <td>1.647511</td>\n",
       "      <td>-0.480963</td>\n",
       "      <td>-1.546866</td>\n",
       "      <td>...</td>\n",
       "      <td>1.252092</td>\n",
       "      <td>-0.993085</td>\n",
       "      <td>-2.173147</td>\n",
       "      <td>0.145570</td>\n",
       "      <td>-0.235062</td>\n",
       "      <td>-0.227411</td>\n",
       "      <td>-0.382702</td>\n",
       "      <td>0.404045</td>\n",
       "      <td>32.002515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117583</th>\n",
       "      <td>0.432480</td>\n",
       "      <td>0.937083</td>\n",
       "      <td>-0.849673</td>\n",
       "      <td>0.524186</td>\n",
       "      <td>-0.020031</td>\n",
       "      <td>-0.606327</td>\n",
       "      <td>0.692302</td>\n",
       "      <td>-0.463724</td>\n",
       "      <td>0.148857</td>\n",
       "      <td>0.785062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143322</td>\n",
       "      <td>-0.479981</td>\n",
       "      <td>-0.237902</td>\n",
       "      <td>-0.715247</td>\n",
       "      <td>0.251418</td>\n",
       "      <td>0.975406</td>\n",
       "      <td>-0.060168</td>\n",
       "      <td>0.023771</td>\n",
       "      <td>2.086495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73349</th>\n",
       "      <td>0.318852</td>\n",
       "      <td>-1.149963</td>\n",
       "      <td>1.696462</td>\n",
       "      <td>1.637114</td>\n",
       "      <td>2.658991</td>\n",
       "      <td>-0.021502</td>\n",
       "      <td>0.192287</td>\n",
       "      <td>0.205204</td>\n",
       "      <td>0.588754</td>\n",
       "      <td>-1.187820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>0.086506</td>\n",
       "      <td>-0.262748</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.210343</td>\n",
       "      <td>-0.162047</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>-0.201495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267336</th>\n",
       "      <td>0.941757</td>\n",
       "      <td>1.754554</td>\n",
       "      <td>-0.699398</td>\n",
       "      <td>-0.076332</td>\n",
       "      <td>0.443915</td>\n",
       "      <td>-0.672082</td>\n",
       "      <td>0.389061</td>\n",
       "      <td>-0.807534</td>\n",
       "      <td>0.202915</td>\n",
       "      <td>0.858635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141950</td>\n",
       "      <td>0.358412</td>\n",
       "      <td>0.259748</td>\n",
       "      <td>0.746839</td>\n",
       "      <td>-0.560808</td>\n",
       "      <td>0.104636</td>\n",
       "      <td>-0.005853</td>\n",
       "      <td>-0.019622</td>\n",
       "      <td>1.017257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128037</th>\n",
       "      <td>0.454743</td>\n",
       "      <td>-0.707635</td>\n",
       "      <td>0.493302</td>\n",
       "      <td>2.648089</td>\n",
       "      <td>1.064807</td>\n",
       "      <td>-0.680271</td>\n",
       "      <td>1.183838</td>\n",
       "      <td>0.169413</td>\n",
       "      <td>0.074553</td>\n",
       "      <td>1.247988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102350</td>\n",
       "      <td>0.323975</td>\n",
       "      <td>-0.172601</td>\n",
       "      <td>0.126965</td>\n",
       "      <td>-0.001998</td>\n",
       "      <td>-0.398741</td>\n",
       "      <td>-0.385589</td>\n",
       "      <td>-0.205589</td>\n",
       "      <td>0.500245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6   \n",
       "169876  0.693938 -0.611712 -0.769705 -0.149759 -0.224877  2.028577 -2.019887  \\\n",
       "127467  0.453377 -0.814682  1.319219  1.329415  0.027273 -0.284871 -0.653985   \n",
       "137900  0.476770 -0.318193  1.118618  0.969864 -0.127052  0.569563 -0.532484   \n",
       "21513   0.183556 -1.328271  1.018378  1.775426 -1.574193 -0.117696 -0.457733   \n",
       "134700  0.468326  1.276712  0.617120 -0.578014  0.879173  0.061706 -1.472002   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "21440   0.183261 -2.986845 -8.663978 -1.910863  0.664058 -3.934875  0.861269   \n",
       "117583  0.432480  0.937083 -0.849673  0.524186 -0.020031 -0.606327  0.692302   \n",
       "73349   0.318852 -1.149963  1.696462  1.637114  2.658991 -0.021502  0.192287   \n",
       "267336  0.941757  1.754554 -0.699398 -0.076332  0.443915 -0.672082  0.389061   \n",
       "128037  0.454743 -0.707635  0.493302  2.648089  1.064807 -0.680271  1.183838   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23   \n",
       "169876  0.292491 -0.523020  0.358468  ... -0.075208  0.045536  0.380739  \\\n",
       "127467  0.321552  0.435975 -0.704298  ... -0.128619 -0.368565  0.090660   \n",
       "137900  0.706252 -0.064966 -0.463271  ... -0.305402 -0.774704 -0.123884   \n",
       "21513   0.681867 -0.031641  0.383872  ... -0.220815 -0.419013 -0.239197   \n",
       "134700  0.373692 -0.287204 -0.084482  ... -0.160161 -0.430404 -0.076738   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "21440   1.647511 -0.480963 -1.546866  ...  1.252092 -0.993085 -2.173147   \n",
       "117583 -0.463724  0.148857  0.785062  ... -0.143322 -0.479981 -0.237902   \n",
       "73349   0.205204  0.588754 -1.187820  ...  0.025147  0.086506 -0.262748   \n",
       "267336 -0.807534  0.202915  0.858635  ...  0.141950  0.358412  0.259748   \n",
       "128037  0.169413  0.074553  1.247988  ... -0.102350  0.323975 -0.172601   \n",
       "\n",
       "             V24       V25       V26       V27       V28     Amount  Class  \n",
       "169876  0.023440 -2.220686 -0.201146  0.066501  0.221180  -0.282401      0  \n",
       "127467  0.401147 -0.261034  0.080621  0.162427  0.059456  -0.279746      0  \n",
       "137900 -0.495687 -0.018148  0.121679  0.249050  0.092516  -0.294977      0  \n",
       "21513   0.009967  0.232829  0.814177  0.098797 -0.004273  -0.084119      0  \n",
       "134700  0.258708  0.552170  0.370701 -0.034255  0.041709  -0.296793      0  \n",
       "...          ...       ...       ...       ...       ...        ...    ...  \n",
       "21440   0.145570 -0.235062 -0.227411 -0.382702  0.404045  32.002515      0  \n",
       "117583 -0.715247  0.251418  0.975406 -0.060168  0.023771   2.086495      0  \n",
       "73349   0.321538  0.341667  0.210343 -0.162047  0.031193  -0.201495      0  \n",
       "267336  0.746839 -0.560808  0.104636 -0.005853 -0.019622   1.017257      0  \n",
       "128037  0.126965 -0.001998 -0.398741 -0.385589 -0.205589   0.500245      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = credit.sample(frac=1, random_state=1)\n",
    "credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41ac77",
   "metadata": {},
   "source": [
    "### Splitting the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc0a7714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Class\n",
       " 0    239589\n",
       " 1       411\n",
       " Name: count, dtype: int64,\n",
       " Class\n",
       " 0    21955\n",
       " 1       45\n",
       " Name: count, dtype: int64,\n",
       " Class\n",
       " 0    22771\n",
       " 1       36\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, val = credit[:240000], credit[240000:262000], credit[262000:]\n",
    "train['Class'].value_counts(), test['Class'].value_counts(), val['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b326272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240000, 31), (22000, 31), (22807, 31))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np, test_np, val_np = train.to_numpy(), test.to_numpy(), val.to_numpy()\n",
    "train_np.shape, test_np.shape, val_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b26f2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240000, 30), (240000,), (22000, 30), (22000,), (22807, 30), (22807,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = train_np[:, :-1], train_np[:, -1]\n",
    "x_test, y_test = test_np[:, :-1], test_np[:, -1]\n",
    "x_val, y_val = val_np[:, :-1], val_np[:, -1]\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48a3d2",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83576b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4c1419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74bee359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     22771\n",
      "         1.0       0.73      0.53      0.61        36\n",
      "\n",
      "    accuracy                           1.00     22807\n",
      "   macro avg       0.87      0.76      0.81     22807\n",
      "weighted avg       1.00      1.00      1.00     22807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d466a5e",
   "metadata": {},
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "704efee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "shallow_nn = Sequential()\n",
    "shallow_nn.add(InputLayer((x_train.shape[1],)))\n",
    "shallow_nn.add(Dense(2, 'relu'))\n",
    "shallow_nn.add(BatchNormalization())\n",
    "shallow_nn.add(Dense(1, 'sigmoid'))\n",
    "\n",
    "checkpoint = ModelCheckpoint('shallow_nn', save_best_only=True)\n",
    "shallow_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c287397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 2)                 62        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 2)                8         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73\n",
      "Trainable params: 69\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "shallow_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc8a27e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9827INFO:tensorflow:Assets written to: shallow_nn\\assets\n",
      "7500/7500 [==============================] - 14s 2ms/step - loss: 0.0537 - accuracy: 0.9827 - val_loss: 0.0046 - val_accuracy: 0.9991\n",
      "Epoch 2/5\n",
      "7487/7500 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9992INFO:tensorflow:Assets written to: shallow_nn\\assets\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
      "Epoch 3/5\n",
      "7489/7500 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9993INFO:tensorflow:Assets written to: shallow_nn\\assets\n",
      "7500/7500 [==============================] - 13s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0034 - val_accuracy: 0.9991\n",
      "Epoch 4/5\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0035 - val_accuracy: 0.9992\n",
      "Epoch 5/5\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0039 - val_accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2343f5e4dc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shallow_nn.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, callbacks=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f76cb0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(shallow_nn.predict(x_train).flatten() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c38ec106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_predictions(model, x):\n",
    "  return (model.predict(x).flatten() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17276241",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = neural_net_predictions(shallow_nn, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "062e716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     22771\n",
      "         1.0       0.65      0.78      0.71        36\n",
      "\n",
      "    accuracy                           1.00     22807\n",
      "   macro avg       0.83      0.89      0.85     22807\n",
      "weighted avg       1.00      1.00      1.00     22807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936707e",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c1d29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "962f1926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     22771\n",
      "         1.0       0.81      0.47      0.60        36\n",
      "\n",
      "    accuracy                           1.00     22807\n",
      "   macro avg       0.90      0.74      0.80     22807\n",
      "weighted avg       1.00      1.00      1.00     22807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf.fit(x_train, y_train)\n",
    "print(classification_report(y_val, rf.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c48414",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b2a8f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.01, max_depth=1, random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=1, random_state=0)\n",
    "gbc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f108ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     22771\n",
      "         1.0       0.73      0.53      0.61        36\n",
      "\n",
      "    accuracy                           1.00     22807\n",
      "   macro avg       0.87      0.76      0.81     22807\n",
      "weighted avg       1.00      1.00      1.00     22807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, gbc.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f4fe1",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fa0c465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(class_weight='balanced')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(class_weight='balanced')\n",
    "svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad74305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     22771\n",
      "         1.0       0.56      0.64      0.60        36\n",
      "\n",
      "    accuracy                           1.00     22807\n",
      "   macro avg       0.78      0.82      0.80     22807\n",
      "weighted avg       1.00      1.00      1.00     22807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, svc.predict(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c888705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169876</th>\n",
       "      <td>0.693938</td>\n",
       "      <td>-0.611712</td>\n",
       "      <td>-0.769705</td>\n",
       "      <td>-0.149759</td>\n",
       "      <td>-0.224877</td>\n",
       "      <td>2.028577</td>\n",
       "      <td>-2.019887</td>\n",
       "      <td>0.292491</td>\n",
       "      <td>-0.523020</td>\n",
       "      <td>0.358468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075208</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.380739</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>-2.220686</td>\n",
       "      <td>-0.201146</td>\n",
       "      <td>0.066501</td>\n",
       "      <td>0.221180</td>\n",
       "      <td>-0.282401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127467</th>\n",
       "      <td>0.453377</td>\n",
       "      <td>-0.814682</td>\n",
       "      <td>1.319219</td>\n",
       "      <td>1.329415</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>-0.284871</td>\n",
       "      <td>-0.653985</td>\n",
       "      <td>0.321552</td>\n",
       "      <td>0.435975</td>\n",
       "      <td>-0.704298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128619</td>\n",
       "      <td>-0.368565</td>\n",
       "      <td>0.090660</td>\n",
       "      <td>0.401147</td>\n",
       "      <td>-0.261034</td>\n",
       "      <td>0.080621</td>\n",
       "      <td>0.162427</td>\n",
       "      <td>0.059456</td>\n",
       "      <td>-0.279746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137900</th>\n",
       "      <td>0.476770</td>\n",
       "      <td>-0.318193</td>\n",
       "      <td>1.118618</td>\n",
       "      <td>0.969864</td>\n",
       "      <td>-0.127052</td>\n",
       "      <td>0.569563</td>\n",
       "      <td>-0.532484</td>\n",
       "      <td>0.706252</td>\n",
       "      <td>-0.064966</td>\n",
       "      <td>-0.463271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305402</td>\n",
       "      <td>-0.774704</td>\n",
       "      <td>-0.123884</td>\n",
       "      <td>-0.495687</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>0.121679</td>\n",
       "      <td>0.249050</td>\n",
       "      <td>0.092516</td>\n",
       "      <td>-0.294977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21513</th>\n",
       "      <td>0.183556</td>\n",
       "      <td>-1.328271</td>\n",
       "      <td>1.018378</td>\n",
       "      <td>1.775426</td>\n",
       "      <td>-1.574193</td>\n",
       "      <td>-0.117696</td>\n",
       "      <td>-0.457733</td>\n",
       "      <td>0.681867</td>\n",
       "      <td>-0.031641</td>\n",
       "      <td>0.383872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220815</td>\n",
       "      <td>-0.419013</td>\n",
       "      <td>-0.239197</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.232829</td>\n",
       "      <td>0.814177</td>\n",
       "      <td>0.098797</td>\n",
       "      <td>-0.004273</td>\n",
       "      <td>-0.084119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134700</th>\n",
       "      <td>0.468326</td>\n",
       "      <td>1.276712</td>\n",
       "      <td>0.617120</td>\n",
       "      <td>-0.578014</td>\n",
       "      <td>0.879173</td>\n",
       "      <td>0.061706</td>\n",
       "      <td>-1.472002</td>\n",
       "      <td>0.373692</td>\n",
       "      <td>-0.287204</td>\n",
       "      <td>-0.084482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160161</td>\n",
       "      <td>-0.430404</td>\n",
       "      <td>-0.076738</td>\n",
       "      <td>0.258708</td>\n",
       "      <td>0.552170</td>\n",
       "      <td>0.370701</td>\n",
       "      <td>-0.034255</td>\n",
       "      <td>0.041709</td>\n",
       "      <td>-0.296793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6   \n",
       "169876  0.693938 -0.611712 -0.769705 -0.149759 -0.224877  2.028577 -2.019887  \\\n",
       "127467  0.453377 -0.814682  1.319219  1.329415  0.027273 -0.284871 -0.653985   \n",
       "137900  0.476770 -0.318193  1.118618  0.969864 -0.127052  0.569563 -0.532484   \n",
       "21513   0.183556 -1.328271  1.018378  1.775426 -1.574193 -0.117696 -0.457733   \n",
       "134700  0.468326  1.276712  0.617120 -0.578014  0.879173  0.061706 -1.472002   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23   \n",
       "169876  0.292491 -0.523020  0.358468  ... -0.075208  0.045536  0.380739  \\\n",
       "127467  0.321552  0.435975 -0.704298  ... -0.128619 -0.368565  0.090660   \n",
       "137900  0.706252 -0.064966 -0.463271  ... -0.305402 -0.774704 -0.123884   \n",
       "21513   0.681867 -0.031641  0.383872  ... -0.220815 -0.419013 -0.239197   \n",
       "134700  0.373692 -0.287204 -0.084482  ... -0.160161 -0.430404 -0.076738   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  Class  \n",
       "169876  0.023440 -2.220686 -0.201146  0.066501  0.221180 -0.282401      0  \n",
       "127467  0.401147 -0.261034  0.080621  0.162427  0.059456 -0.279746      0  \n",
       "137900 -0.495687 -0.018148  0.121679  0.249050  0.092516 -0.294977      0  \n",
       "21513   0.009967  0.232829  0.814177  0.098797 -0.004273 -0.084119      0  \n",
       "134700  0.258708  0.552170  0.370701 -0.034255  0.041709 -0.296793      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcea7913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Class\n",
       " 0    284315\n",
       " Name: count, dtype: int64,\n",
       " Class\n",
       " 1    492\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_frauds = credit.query('Class == 0')\n",
    "frauds = credit.query('Class == 1')\n",
    "not_frauds['Class'].value_counts(), frauds['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62abf84",
   "metadata": {},
   "source": [
    "### Handling imbalanced dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbab2135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "1    492\n",
       "0    492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df = pd.concat([frauds, not_frauds.sample(len(frauds), random_state=1)])\n",
    "balanced_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d626b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18372</th>\n",
       "      <td>0.170309</td>\n",
       "      <td>-1.762593</td>\n",
       "      <td>0.256143</td>\n",
       "      <td>1.683125</td>\n",
       "      <td>-1.279233</td>\n",
       "      <td>-1.902762</td>\n",
       "      <td>1.004210</td>\n",
       "      <td>-1.009748</td>\n",
       "      <td>-2.432546</td>\n",
       "      <td>0.458860</td>\n",
       "      <td>...</td>\n",
       "      <td>2.493579</td>\n",
       "      <td>0.320829</td>\n",
       "      <td>-0.535481</td>\n",
       "      <td>0.499401</td>\n",
       "      <td>-0.915196</td>\n",
       "      <td>-0.423434</td>\n",
       "      <td>0.107049</td>\n",
       "      <td>0.175922</td>\n",
       "      <td>2.906449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96341</th>\n",
       "      <td>0.380388</td>\n",
       "      <td>1.227614</td>\n",
       "      <td>-0.668974</td>\n",
       "      <td>-0.271785</td>\n",
       "      <td>-0.589440</td>\n",
       "      <td>-0.604795</td>\n",
       "      <td>-0.350285</td>\n",
       "      <td>-0.486365</td>\n",
       "      <td>-0.010809</td>\n",
       "      <td>-0.794944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026055</td>\n",
       "      <td>-0.295255</td>\n",
       "      <td>-0.180459</td>\n",
       "      <td>-0.436539</td>\n",
       "      <td>0.494649</td>\n",
       "      <td>-0.283738</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.035075</td>\n",
       "      <td>1.062111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248296</th>\n",
       "      <td>0.890522</td>\n",
       "      <td>-0.613696</td>\n",
       "      <td>3.698772</td>\n",
       "      <td>-5.534941</td>\n",
       "      <td>5.620486</td>\n",
       "      <td>1.649263</td>\n",
       "      <td>-2.335145</td>\n",
       "      <td>-0.907188</td>\n",
       "      <td>0.706362</td>\n",
       "      <td>-3.747646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319261</td>\n",
       "      <td>-0.471379</td>\n",
       "      <td>-0.075890</td>\n",
       "      <td>-0.667909</td>\n",
       "      <td>-0.642848</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.488410</td>\n",
       "      <td>0.292345</td>\n",
       "      <td>-0.307413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264328</th>\n",
       "      <td>0.933932</td>\n",
       "      <td>-0.011624</td>\n",
       "      <td>0.640413</td>\n",
       "      <td>0.868046</td>\n",
       "      <td>-0.505279</td>\n",
       "      <td>0.261938</td>\n",
       "      <td>0.223098</td>\n",
       "      <td>0.239049</td>\n",
       "      <td>0.150877</td>\n",
       "      <td>0.225142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069401</td>\n",
       "      <td>0.268024</td>\n",
       "      <td>0.261459</td>\n",
       "      <td>0.683742</td>\n",
       "      <td>-1.567901</td>\n",
       "      <td>-0.816674</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.283021</td>\n",
       "      <td>-0.272619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208904</th>\n",
       "      <td>0.794730</td>\n",
       "      <td>-0.679341</td>\n",
       "      <td>1.217389</td>\n",
       "      <td>-0.316778</td>\n",
       "      <td>-1.086725</td>\n",
       "      <td>0.855349</td>\n",
       "      <td>-0.980760</td>\n",
       "      <td>0.970589</td>\n",
       "      <td>0.133116</td>\n",
       "      <td>-0.357671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083048</td>\n",
       "      <td>-0.137032</td>\n",
       "      <td>-0.238920</td>\n",
       "      <td>-0.617244</td>\n",
       "      <td>0.039020</td>\n",
       "      <td>-0.081848</td>\n",
       "      <td>0.234633</td>\n",
       "      <td>0.128382</td>\n",
       "      <td>-0.307273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81557</th>\n",
       "      <td>0.341393</td>\n",
       "      <td>-4.502731</td>\n",
       "      <td>-3.876484</td>\n",
       "      <td>1.341248</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.189428</td>\n",
       "      <td>-0.560985</td>\n",
       "      <td>-0.140478</td>\n",
       "      <td>0.684651</td>\n",
       "      <td>0.475363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140218</td>\n",
       "      <td>0.049411</td>\n",
       "      <td>2.313731</td>\n",
       "      <td>0.252330</td>\n",
       "      <td>0.307219</td>\n",
       "      <td>0.859051</td>\n",
       "      <td>0.184033</td>\n",
       "      <td>-0.308269</td>\n",
       "      <td>4.227625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276071</th>\n",
       "      <td>0.965803</td>\n",
       "      <td>2.091900</td>\n",
       "      <td>-0.757459</td>\n",
       "      <td>-1.192258</td>\n",
       "      <td>-0.755458</td>\n",
       "      <td>-0.620324</td>\n",
       "      <td>-0.322077</td>\n",
       "      <td>-1.082511</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>-0.140927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288253</td>\n",
       "      <td>0.831939</td>\n",
       "      <td>0.142007</td>\n",
       "      <td>0.592615</td>\n",
       "      <td>-0.196143</td>\n",
       "      <td>-0.136676</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>-0.015470</td>\n",
       "      <td>-0.028645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175971</th>\n",
       "      <td>0.709373</td>\n",
       "      <td>1.972989</td>\n",
       "      <td>0.157281</td>\n",
       "      <td>-1.715078</td>\n",
       "      <td>1.207451</td>\n",
       "      <td>0.681612</td>\n",
       "      <td>-0.615282</td>\n",
       "      <td>0.601791</td>\n",
       "      <td>-0.291935</td>\n",
       "      <td>-0.132265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098640</td>\n",
       "      <td>0.467533</td>\n",
       "      <td>-0.078973</td>\n",
       "      <td>-0.371882</td>\n",
       "      <td>0.486038</td>\n",
       "      <td>-0.490665</td>\n",
       "      <td>-0.018374</td>\n",
       "      <td>-0.070911</td>\n",
       "      <td>0.075735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27738</th>\n",
       "      <td>0.200727</td>\n",
       "      <td>-2.439237</td>\n",
       "      <td>2.591458</td>\n",
       "      <td>-2.840126</td>\n",
       "      <td>1.286244</td>\n",
       "      <td>-1.777016</td>\n",
       "      <td>-1.436139</td>\n",
       "      <td>-2.206056</td>\n",
       "      <td>-2.282725</td>\n",
       "      <td>-0.292885</td>\n",
       "      <td>...</td>\n",
       "      <td>1.774460</td>\n",
       "      <td>-0.771390</td>\n",
       "      <td>0.065727</td>\n",
       "      <td>0.103916</td>\n",
       "      <td>-0.057578</td>\n",
       "      <td>0.242652</td>\n",
       "      <td>-0.268649</td>\n",
       "      <td>-0.743713</td>\n",
       "      <td>1.443443</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156988</th>\n",
       "      <td>0.632535</td>\n",
       "      <td>0.745153</td>\n",
       "      <td>2.809299</td>\n",
       "      <td>-5.825406</td>\n",
       "      <td>5.835566</td>\n",
       "      <td>0.512320</td>\n",
       "      <td>-0.615622</td>\n",
       "      <td>-2.916576</td>\n",
       "      <td>0.776710</td>\n",
       "      <td>-1.878832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284841</td>\n",
       "      <td>-0.874383</td>\n",
       "      <td>-0.083995</td>\n",
       "      <td>-0.651442</td>\n",
       "      <td>0.454594</td>\n",
       "      <td>0.050376</td>\n",
       "      <td>0.756953</td>\n",
       "      <td>0.383869</td>\n",
       "      <td>-0.307413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6   \n",
       "18372   0.170309 -1.762593  0.256143  1.683125 -1.279233 -1.902762  1.004210  \\\n",
       "96341   0.380388  1.227614 -0.668974 -0.271785 -0.589440 -0.604795 -0.350285   \n",
       "248296  0.890522 -0.613696  3.698772 -5.534941  5.620486  1.649263 -2.335145   \n",
       "264328  0.933932 -0.011624  0.640413  0.868046 -0.505279  0.261938  0.223098   \n",
       "208904  0.794730 -0.679341  1.217389 -0.316778 -1.086725  0.855349 -0.980760   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "81557   0.341393 -4.502731 -3.876484  1.341248  0.113400  0.189428 -0.560985   \n",
       "276071  0.965803  2.091900 -0.757459 -1.192258 -0.755458 -0.620324 -0.322077   \n",
       "175971  0.709373  1.972989  0.157281 -1.715078  1.207451  0.681612 -0.615282   \n",
       "27738   0.200727 -2.439237  2.591458 -2.840126  1.286244 -1.777016 -1.436139   \n",
       "156988  0.632535  0.745153  2.809299 -5.825406  5.835566  0.512320 -0.615622   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23   \n",
       "18372  -1.009748 -2.432546  0.458860  ...  2.493579  0.320829 -0.535481  \\\n",
       "96341  -0.486365 -0.010809 -0.794944  ... -0.026055 -0.295255 -0.180459   \n",
       "248296 -0.907188  0.706362 -3.747646  ...  0.319261 -0.471379 -0.075890   \n",
       "264328  0.239049  0.150877  0.225142  ...  0.069401  0.268024  0.261459   \n",
       "208904  0.970589  0.133116 -0.357671  ... -0.083048 -0.137032 -0.238920   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "81557  -0.140478  0.684651  0.475363  ... -0.140218  0.049411  2.313731   \n",
       "276071 -1.082511  0.117200 -0.140927  ...  0.288253  0.831939  0.142007   \n",
       "175971  0.601791 -0.291935 -0.132265  ...  0.098640  0.467533 -0.078973   \n",
       "27738  -2.206056 -2.282725 -0.292885  ...  1.774460 -0.771390  0.065727   \n",
       "156988 -2.916576  0.776710 -1.878832  ...  0.284841 -0.874383 -0.083995   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  Class  \n",
       "18372   0.499401 -0.915196 -0.423434  0.107049  0.175922  2.906449      0  \n",
       "96341  -0.436539  0.494649 -0.283738 -0.001128  0.035075  1.062111      1  \n",
       "248296 -0.667909 -0.642848  0.070600  0.488410  0.292345 -0.307413      1  \n",
       "264328  0.683742 -1.567901 -0.816674  0.185781  0.283021 -0.272619      0  \n",
       "208904 -0.617244  0.039020 -0.081848  0.234633  0.128382 -0.307273      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "81557   0.252330  0.307219  0.859051  0.184033 -0.308269  4.227625      0  \n",
       "276071  0.592615 -0.196143 -0.136676  0.020182 -0.015470 -0.028645      1  \n",
       "175971 -0.371882  0.486038 -0.490665 -0.018374 -0.070911  0.075735      0  \n",
       "27738   0.103916 -0.057578  0.242652 -0.268649 -0.743713  1.443443      1  \n",
       "156988 -0.651442  0.454594  0.050376  0.756953  0.383869 -0.307413      1  \n",
       "\n",
       "[984 rows x 31 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df = balanced_df.sample(frac=1, random_state=1)\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0eee0e",
   "metadata": {},
   "source": [
    "### Splitting the balanced dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf81918",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df_np = balanced_df.to_numpy()\n",
    "\n",
    "x_train_b, y_train_b = balanced_df_np[:700, :-1], balanced_df_np[:700, -1].astype(int)\n",
    "x_test_b, y_test_b = balanced_df_np[700:842, :-1], balanced_df_np[700:842, -1].astype(int)\n",
    "x_val_b, y_val_b = balanced_df_np[842:, :-1], balanced_df_np[842:, -1].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62421689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 30), (700,), (142, 30), (142,), (142, 30), (142,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_b.shape, y_train_b.shape, x_test_b.shape, y_test_b.shape, x_val_b.shape, y_val_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train_b).value_counts(), pd.Series(y_test_b).value_counts(), pd.Series(y_val_b).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4eb8a",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19ceacbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model_b = LogisticRegression()\n",
    "logistic_model_b.fit(x_train_b, y_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eeb98dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        72\n",
      "           1       0.93      0.96      0.94        70\n",
      "\n",
      "    accuracy                           0.94       142\n",
      "   macro avg       0.94      0.94      0.94       142\n",
      "weighted avg       0.94      0.94      0.94       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_b, logistic_model_b.predict(x_val_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bec8ca",
   "metadata": {},
   "source": [
    "### Randon forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef520246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=2, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=2, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=2, n_jobs=-1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_b = RandomForestClassifier(max_depth=2, n_jobs=-1)\n",
    "rf_b.fit(x_train_b, y_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f1b79c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82        72\n",
      "           1       1.00      0.54      0.70        70\n",
      "\n",
      "    accuracy                           0.77       142\n",
      "   macro avg       0.85      0.77      0.76       142\n",
      "weighted avg       0.84      0.77      0.76       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_b, rf.predict(x_val_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eff625",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b7925bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=1.0, max_depth=2, n_estimators=50,\n",
       "                           random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=1.0, max_depth=2, n_estimators=50,\n",
       "                           random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=2, n_estimators=50,\n",
       "                           random_state=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_b = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, max_depth=2, random_state=0)\n",
    "gbc_b.fit(x_train_b, y_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1acf741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81        72\n",
      "           1       1.00      0.51      0.68        70\n",
      "\n",
      "    accuracy                           0.76       142\n",
      "   macro avg       0.84      0.76      0.74       142\n",
      "weighted avg       0.84      0.76      0.75       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_b, gbc.predict(x_val_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e011251",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a6301c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(class_weight='balanced')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_b = LinearSVC(class_weight='balanced')\n",
    "svc_b.fit(x_train_b, y_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "530a93cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86        72\n",
      "           1       1.00      0.66      0.79        70\n",
      "\n",
      "    accuracy                           0.83       142\n",
      "   macro avg       0.88      0.83      0.83       142\n",
      "weighted avg       0.87      0.83      0.83       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_b, svc.predict(x_val_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85d07b",
   "metadata": {},
   "source": [
    "### Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa533e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_nn_b = Sequential()\n",
    "shallow_nn_b.add(InputLayer((x_train.shape[1],)))\n",
    "shallow_nn_b.add(Dense(2, 'relu'))\n",
    "shallow_nn_b.add(BatchNormalization())\n",
    "shallow_nn_b.add(Dense(1, 'sigmoid'))\n",
    "\n",
    "checkpoint = ModelCheckpoint('shallow_nn_b', save_best_only=True)\n",
    "shallow_nn_b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "381ad9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.7697 - accuracy: 0.4109 INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 2s 59ms/step - loss: 0.7669 - accuracy: 0.4186 - val_loss: 0.7038 - val_accuracy: 0.5704\n",
      "Epoch 2/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.7623 - accuracy: 0.4688INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6889 - accuracy: 0.5757 - val_loss: 0.6600 - val_accuracy: 0.7113\n",
      "Epoch 3/50\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6412 - accuracy: 0.6756INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6412 - accuracy: 0.6757 - val_loss: 0.6470 - val_accuracy: 0.7394\n",
      "Epoch 4/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6385 - accuracy: 0.6875INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6300 - accuracy: 0.7129 - val_loss: 0.6370 - val_accuracy: 0.7676\n",
      "Epoch 5/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5883 - accuracy: 0.7812INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 67ms/step - loss: 0.6207 - accuracy: 0.7257 - val_loss: 0.6270 - val_accuracy: 0.7606\n",
      "Epoch 6/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6528 - accuracy: 0.6250INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.6109 - accuracy: 0.7386 - val_loss: 0.6160 - val_accuracy: 0.7606\n",
      "Epoch 7/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6095 - accuracy: 0.7188INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.6000 - accuracy: 0.7486 - val_loss: 0.6041 - val_accuracy: 0.7606\n",
      "Epoch 8/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.6254 - accuracy: 0.7188INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5904 - accuracy: 0.7500 - val_loss: 0.5913 - val_accuracy: 0.7746\n",
      "Epoch 9/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5350 - accuracy: 0.8438INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5809 - accuracy: 0.7686 - val_loss: 0.5781 - val_accuracy: 0.7817\n",
      "Epoch 10/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5766 - accuracy: 0.7812INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.5710 - accuracy: 0.7729 - val_loss: 0.5634 - val_accuracy: 0.8028\n",
      "Epoch 11/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5183 - accuracy: 0.8438INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.5536 - accuracy: 0.7900 - val_loss: 0.5476 - val_accuracy: 0.8028\n",
      "Epoch 12/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5758 - accuracy: 0.7500INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5425 - accuracy: 0.7957 - val_loss: 0.5308 - val_accuracy: 0.8380\n",
      "Epoch 13/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5167 - accuracy: 0.8438INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.5278 - accuracy: 0.8071 - val_loss: 0.5134 - val_accuracy: 0.8310\n",
      "Epoch 14/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5391 - accuracy: 0.7188INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.5134 - accuracy: 0.8086 - val_loss: 0.4954 - val_accuracy: 0.8521\n",
      "Epoch 15/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.5164 - accuracy: 0.8438INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.4946 - accuracy: 0.8329 - val_loss: 0.4769 - val_accuracy: 0.8732\n",
      "Epoch 16/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4647 - accuracy: 0.9375INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.4734 - accuracy: 0.8486 - val_loss: 0.4564 - val_accuracy: 0.8873\n",
      "Epoch 17/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4735 - accuracy: 0.7812INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.4524 - accuracy: 0.8643 - val_loss: 0.4365 - val_accuracy: 0.8944\n",
      "Epoch 18/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4195 - accuracy: 0.9062INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4407 - accuracy: 0.8600 - val_loss: 0.4163 - val_accuracy: 0.9155\n",
      "Epoch 19/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4439 - accuracy: 0.8125INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.4119 - accuracy: 0.8771 - val_loss: 0.3967 - val_accuracy: 0.9155\n",
      "Epoch 20/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4168 - accuracy: 0.8438INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.3942 - accuracy: 0.8814 - val_loss: 0.3775 - val_accuracy: 0.9225\n",
      "Epoch 21/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4011 - accuracy: 0.8750INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.3682 - accuracy: 0.8986 - val_loss: 0.3574 - val_accuracy: 0.9225\n",
      "Epoch 22/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4005 - accuracy: 0.7812INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.3567 - accuracy: 0.8971 - val_loss: 0.3393 - val_accuracy: 0.9225\n",
      "Epoch 23/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2713 - accuracy: 0.9375INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.3424 - accuracy: 0.9057 - val_loss: 0.3219 - val_accuracy: 0.9155\n",
      "Epoch 24/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.3885 - accuracy: 0.9062INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.3213 - accuracy: 0.9143 - val_loss: 0.3061 - val_accuracy: 0.9155\n",
      "Epoch 25/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.3965 - accuracy: 0.8438INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.3129 - accuracy: 0.9186 - val_loss: 0.2913 - val_accuracy: 0.9296\n",
      "Epoch 26/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8438INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.2890 - accuracy: 0.9286 - val_loss: 0.2776 - val_accuracy: 0.9296\n",
      "Epoch 27/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2293 - accuracy: 0.9688INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.2832 - accuracy: 0.9257 - val_loss: 0.2664 - val_accuracy: 0.9296\n",
      "Epoch 28/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2770 - accuracy: 0.9062INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.2707 - accuracy: 0.9271 - val_loss: 0.2560 - val_accuracy: 0.9366\n",
      "Epoch 29/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4294 - accuracy: 0.8750INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.2595 - accuracy: 0.9286 - val_loss: 0.2459 - val_accuracy: 0.9366\n",
      "Epoch 30/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2338 - accuracy: 0.9688INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 33ms/step - loss: 0.2492 - accuracy: 0.9371 - val_loss: 0.2378 - val_accuracy: 0.9366\n",
      "Epoch 31/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2061 - accuracy: 0.9375INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.2340 - accuracy: 0.9429 - val_loss: 0.2308 - val_accuracy: 0.9296\n",
      "Epoch 32/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.1943 - accuracy: 0.9688INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.2390 - accuracy: 0.9414 - val_loss: 0.2252 - val_accuracy: 0.9296\n",
      "Epoch 33/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2148 - accuracy: 0.8750INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.2372 - accuracy: 0.9257 - val_loss: 0.2193 - val_accuracy: 0.9296\n",
      "Epoch 34/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2192 - accuracy: 0.9688INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.2151 - accuracy: 0.9371 - val_loss: 0.2146 - val_accuracy: 0.9296\n",
      "Epoch 35/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.4654 - accuracy: 0.9062INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.2220 - accuracy: 0.9371 - val_loss: 0.2104 - val_accuracy: 0.9296\n",
      "Epoch 36/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2724 - accuracy: 0.9375INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.2137 - accuracy: 0.9400 - val_loss: 0.2054 - val_accuracy: 0.9296\n",
      "Epoch 37/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2245 - accuracy: 0.9375INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.2082 - accuracy: 0.9443 - val_loss: 0.2019 - val_accuracy: 0.9296\n",
      "Epoch 38/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2286 - accuracy: 0.9062INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.2015 - accuracy: 0.9429 - val_loss: 0.1987 - val_accuracy: 0.9296\n",
      "Epoch 39/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.3242 - accuracy: 0.9062INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.1937 - accuracy: 0.9486 - val_loss: 0.1960 - val_accuracy: 0.9366\n",
      "Epoch 40/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.1721 - accuracy: 1.0000INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.1836 - accuracy: 0.9443 - val_loss: 0.1937 - val_accuracy: 0.9366\n",
      "Epoch 41/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.3314 - accuracy: 0.8750INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.1928 - accuracy: 0.9471 - val_loss: 0.1932 - val_accuracy: 0.9366\n",
      "Epoch 42/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.2772 - accuracy: 0.8750INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.1768 - accuracy: 0.9486 - val_loss: 0.1919 - val_accuracy: 0.9366\n",
      "Epoch 43/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.1527 - accuracy: 0.9688INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.1844 - accuracy: 0.9543 - val_loss: 0.1915 - val_accuracy: 0.9296\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1680 - accuracy: 0.9543 - val_loss: 0.1915 - val_accuracy: 0.9296\n",
      "Epoch 45/50\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.1203 - accuracy: 0.9688INFO:tensorflow:Assets written to: shallow_nn_b\\assets\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.1821 - accuracy: 0.9443 - val_loss: 0.1892 - val_accuracy: 0.9296\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1783 - accuracy: 0.9471 - val_loss: 0.1910 - val_accuracy: 0.9296\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1674 - accuracy: 0.9514 - val_loss: 0.1911 - val_accuracy: 0.9296\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1598 - accuracy: 0.9557 - val_loss: 0.1912 - val_accuracy: 0.9296\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1684 - accuracy: 0.9443 - val_loss: 0.1905 - val_accuracy: 0.9296\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1652 - accuracy: 0.9500 - val_loss: 0.1904 - val_accuracy: 0.9366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23454d82910>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shallow_nn_b.fit(x_train_b, y_train_b, validation_data=(x_val_b, y_val_b), epochs=50, callbacks=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c899de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94        72\n",
      "           1       0.93      0.94      0.94        70\n",
      "\n",
      "    accuracy                           0.94       142\n",
      "   macro avg       0.94      0.94      0.94       142\n",
      "weighted avg       0.94      0.94      0.94       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_b, neural_net_predictions(shallow_nn_b, x_val_b)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
